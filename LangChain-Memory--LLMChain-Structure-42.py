from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains.llm import LLMChain
from langchain_core.messages import SystemMessage
from langchain_core.prompts import MessagesPlaceholder, HumanMessagePromptTemplate, ChatPromptTemplate
from dotenv import load_dotenv
load_dotenv()



# å¦‚æœä½¿ç”¨èŠå¤©æ¨¡å‹ï¼Œä½¿ç”¨ç»“æ„åŒ–çš„èŠå¤©æ¶ˆæ¯å¯èƒ½ä¼šæœ‰æ›´å¥½çš„æ€§èƒ½ã€‚
# åˆå§‹åŒ–å¤§æ¨¡å‹
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

# ä½¿ç”¨ChatPromptTemplateè®¾ç½®èŠå¤©æç¤º
prompt = ChatPromptTemplate.from_messages( # ï¼ˆæ›´é€‚åˆ chat æ¨¡å‹ï¼‰
    [
        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸äººç±»å¯¹è¯çš„æœºå™¨äººã€‚"),
        MessagesPlaceholder(variable_name="chat_history"),
        HumanMessagePromptTemplate.from_template("{question}"),
    ]
)
# print(prompt)

# åˆ›å»ºConversationBufferMemory
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# åˆå§‹åŒ–é“¾
chain = LLMChain(llm=llm,  prompt=prompt, memory=memory)

# æé—®
res = chain.invoke({"question": "ä½ æ˜¯LangChainä¸“å®¶"})
print(str(res) + "\n")

res = chain.invoke({"question": "ä½ æ˜¯è°?"})
print(res)


# ğŸ§ª å“ªä¸ªæ›´å¼ºï¼Ÿ
# ChatPromptTemplate + ChatOpenAI ç»„åˆï¼š
# æ”¯æŒå¤šè½®å¯¹è¯ç»“æ„åŒ–è®°å¿†
# èƒ½åŒºåˆ† systemã€userã€assistant è§’è‰²
# å“åº”æ›´è‡ªç„¶ã€æ›´ç¨³å®š
# æ˜¯æœªæ¥ä¸»æµæ¨èæ–¹å¼